{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD2qY53+elg7nypoac575P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarianVelasquez/Sentiment-Analysis-and-Text-Mining/blob/main/Assigment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9xHoOAriRAX"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Load dataset with top 10,000 words\n",
        "num_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}, Test samples: {len(x_test)}\")\n",
        "\n",
        "# Get the word index mapping\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "\n",
        "def decode_review(text_ids):\n",
        "    # Note: Keras IMDB reserves indices 0-3 for special tokens\n",
        "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in text_ids])\n",
        "\n",
        "# Show a sample decoded review\n",
        "print(\"Sample review:\", decode_review(x_train[0]))\n",
        "print(\"Label:\", y_train[0])\n",
        "\n",
        "## 2. Text Preprocessing\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and digits\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Decode and clean a subset (for speed in demo)\n",
        "train_texts = [clean_text(decode_review(x)) for x in x_train[:5000]]\n",
        "test_texts = [clean_text(decode_review(x)) for x in x_test[:1000]]\n",
        "train_labels = y_train[:5000]\n",
        "test_labels = y_test[:1000]\n",
        "\n",
        "print(\"Sample cleaned text:\", train_texts[0])\n",
        "\n",
        "\n",
        "## 3. Feature Extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X_train = vectorizer.fit_transform(train_texts)\n",
        "X_test = vectorizer.transform(test_texts)\n",
        "\n",
        "print(\"TF-IDF matrix shape:\", X_train.shape)\n",
        "\n",
        "## 4. Model Training and Evaluation\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train, train_labels)\n",
        "lr_preds = lr.predict(X_test)\n",
        "\n",
        "# Naive Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, train_labels)\n",
        "nb_preds = nb.predict(X_test)\n",
        "\n",
        "# SVM\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, train_labels)\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(test_labels, lr_preds))\n",
        "print(\"Naive Bayes Accuracy:\", accuracy_score(test_labels, nb_preds))\n",
        "print(\"SVM Accuracy:\", accuracy_score(test_labels, svm_preds))\n",
        "\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(test_labels, lr_preds))\n",
        "\n",
        "## 5. Pipeline and Hyperparameter Tuning\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1,2))),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "params = {\n",
        "    'tfidf__max_features': [3000, 5000],\n",
        "    'clf__C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "grid.fit(train_texts, train_labels)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best CV Score:\", grid.best_score_)\n",
        "\n",
        "## 6. Inference on New Data\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "sample_reviews = [\n",
        "    \"I absolutely loved this movie, it was fantastic!\",\n",
        "    \"Terrible movie, waste of time.\",\n",
        "    \"It was okay, not the best but not the worst.\",\n",
        "    \"Brilliant acting and great storyline.\",\n",
        "    \"Worst plot ever, very disappointing.\"\n",
        "]\n",
        "\n",
        "predictions = best_model.predict(sample_reviews)\n",
        "\n",
        "for review, pred in zip(sample_reviews, predictions):\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {'Positive' if pred == 1 else 'Negative'}\\n\")"
      ],
      "metadata": {
        "id": "BShZ00eKiVMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}